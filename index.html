<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="author" content="OrionLab" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="keywords" content="Rui Shao, Shao Rui, OrionLab, HIT, Harbin Institute of Technology, Shenzhen" />
    <title>Rui Shao - Harbin Institute of Technology (Shenzhen)</title>
    <link href="static/bootstrap/css/bootstrap.css" rel="stylesheet" />
    <link href="static/xin.css" rel="stylesheet" />
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-static-top">
      <div class="container">
        <div class="navbar-header">
          <button
            type="button"
            class="navbar-toggle"
            data-toggle="collapse"
            data-target=".navbar-collapse"
          >
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <!-- <span class="navbar-brand text-white">OrionLab</span> -->
          <span class="navbar-brand orionlab-brand">OrionLab</span>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="index.html">Home</a></li>
            <li><a href="publications/index.html">Publications</a></li>
            <li><a href="projects/index.html">Projects</a></li>
            <li><a href="team/index.html">Team</a></li>
          </ul>
        </div>
      </div>
    </nav>

    <div class="container">
  <div class="row" style="margin-top: 30px">
    <!-- 左侧头像 -->
    <div class="col-sm-3">
      <img
        src="home_images/shaorui.jpg"
        class="img"
        style="width: 160px; margin-top: 10px; border-radius: 10px"
        alt="Rui Shao Portrait"
      />
    </div>

    <!-- 右侧信息 -->
    <div class="col-sm-9">
      <h1>Rui Shao (邵睿)</h1>
      <h3>Professor</h3>
      <p>
        <a href="http://cs.hitsz.edu.cn/">School of Computer Science and Technology</a><br />
        <a href="https://www.hitsz.edu.cn/index.html">Harbin Institute of Technology (Shenzhen)</a>
      </p>
      <p>Email: shaorui[AT]hit.edu.cn, rshaojimmy[AT]gmail.com</p>
      <p>
        [<a href="http://faculty.hitsz.edu.cn/shaorui">中文主页</a>]
        [<a href="https://scholar.google.com/citations?hl=en&user=9Vc--XsAAAAJ">Google Scholar</a>]
        [<a href="https://dblp.org/pid/182/4717-1.html">DBLP</a>]
        [<a href="https://github.com/rshaojimmy">GitHub</a>]
        <!-- [<a href="https://www.linkedin.com/in/rui-shao-0623b6122/">LinkedIn</a>]-->
      </p>
    </div>
  </div>

  <p>
    Welcome to the multimOdal peRception, reasonIng, and decisiON (Orion) Lab at HIT(SZ), led by Prof. Rui Shao.
    
    We study intelligent agents based on Multimodal Large Language Model (MLLM) that can perceive, reason, and act through interaction with the world. 
  </p>

  <p style="color: red; font-style: italic">
    Looking for self-motivated Ph.D/M.S./Undergraduate students. [2026硕士/博士招生, 3-4名硕士, 2名博士]
  </p>
  <p style="color: red; font-style: italic">
    Looking for PostDocs in MLLM, Embodied AI and Agent.
  </p>
<!-- </div> -->


      <hr>
      <h2>News</h2>
      <div style="height: 250px; overflow-y: auto;">
        <ul style="padding-left: 20px; margin-top: 5px;">
        <li>06/2025: Three papers about MLLM, AI Agent are accepted by <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
        <li>06/2025: One paper about Audio-Visual Multimodal Large Language Model is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"> TPAMI</a>.
        <li>05/2025: One paper about GUI agent is accepted by <a href="https://2025.aclweb.org/">ACL Main 2025</a> .
        <li>05/2025: Invited to serve as ICMR 2025 Panel Co-Chairs and BMVC 2025 Area Chair.
        <li>05/2025: One paper about Robot Skill Learning is accepted by <a href="https://icml.cc/">ICML 2025</a> as Spotlight (2.6%).   
        <li>02/2025: Three papers about Ego-Centric video MLLM, MLLM agent and Embodied MLLM are accepted by <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.                                                
        <li>01/2025: One paper about SmartPhone Multimodal Agent accepted by <a href="https://iclr.cc//">ICLR 2025</a> as Spotlight (5.1%).                        
        <li>12/2024: The extension of our ECCV 2022 paper (SeqDeepFake) has been accepted by <a href="https://www.springer.com/journal/11263/"> IJCV</a>. </a>
        <li>10/2024: We have built GitHub Orgnization of <a href="https://github.com/JiuTian-VL">JiuTian-VL</a> that will post all information about our JiuTian MLLM.
        <li>10/2024: I have one paper about the adapter of large vision models accepted by <a href="https://www.springer.com/journal/11263/"> IJCV</a>.
        <li>10/2024: Two papers about MLLMs are accepted at <a href="https://neurips.cc/">NeurIPS 2024</a>, including contributions from a 2nd-year undergraduate.
        <li>07/2024: One paper about Audio-Visual Multimodal Large Language Model accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
        <li>02/2024: Our Multimodal Large Language Model (MLLM)-<a href="https://arxiv.org/pdf/2311.11860.pdf"> JiuTian-LION </a> has been accepted by <a href="https://cvpr.thecvf.com/"> CVPR 2024</a>.
        <li>02/2024: The extension of our <a href="https://github.com/rshaojimmy/MultiModal-DeepFake">CVPR 2023 paper</a> has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"> TPAMI</a>.
        <li>08/2023: We have built the GitHub Repo for our Multimodal Large Language Model (MLLM)-<a href="https://github.com/rshaojimmy/JiuTian"> JiuTian </a>. Enjoy it!
        <li>04/2023: I have released the code and dataset of our CVPR 2023 work in our <a href="https://github.com/rshaojimmy/MultiModal-DeepFake"> GitHub Repo </a>. Enjoy it!
        <li>02/2023: I have one paper accepted by <a href="https://cvpr2023.thecvf.com/"> CVPR 2023</a>. Code and dataset will be released soon. Please stay tuned! </a>
        <li>07/2022: I have one paper accepted by <a href="https://eccv2022.ecva.net/"> ECCV 2022</a>. We have released the code and dataset in our <a href="https://rshaojimmy.github.io/Projects/SeqDeepFake"> project page </a>
        <li>05/2022: I have released the code of Federated Generalized Face Presentation Attack Detection in TNNLS 2022. <a href="https://github.com/rshaojimmy/TNNLS2022-FedGPAD"> Codes </a>
        <li>04/2022: I have one paper accepted by <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems"> IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </a>.
        <li>03/2022: I have released the code of Open-set Adversarial Defense with Clean-Adversarial Mutual Learning in IJCV 2022. <a href="https://github.com/rshaojimmy/IJCV2022-OSDN-CAML"> Codes </a>
        <li>01/2022: The extension of our ECCV 2020 paper has been accepted by <a href="https://www.springer.com/journal/11263/"> IJCV. </a>
        <li>08/2020: I have released the code of Open-set Adversarial Defense in ECCV 2020. <a href="https://github.com/rshaojimmy/ECCV2020-OSAD"> Codes </a>
        <li>07/2020: I have one paper accepted by <a href="https://eccv2020.eu/"> ECCV 2020</a>. See you online!  
        <li>11/2019: I have one paper accepted by <a href="https://aaai.org/Conferences/AAAI-20/#"> AAAI 2020</a>. See you at New York City, USA!                         
        <li>10/2019: I have one paper one paper accepted by <a href="https://digital-library.theiet.org/content/journals/iet-ipr"> IET Image Processing. </a>
        <li>02/2019: I have one paper accepted by <a href="http://cvpr2019.thecvf.com/"> CVPR 2019</a>. See you at Long Beach, USA!                          
        <li>02/2019: One paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=41"> IEEE Transactions on Industrial Electronics. </a> 
        <li>08/2018: I have one paper accepted by <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security"> IEEE Transactions on Information Forensics and Security </a>.
        <li>07/2018: I have released the code of Hierarchical Adversarial Deep Domain Adaptation in ACMMM 2018. <a href="https://github.com/rshaojimmy/ACMMM2018-HADDA"> Codes </a>
        <li>07/2018: I have one paper accepted by <a href="http://www.acmmm.org/2018/"> ACM MM 2018</a>. See you at Seoul, Korea!                            
        <li>08/2018: I have a new homepage.
        </ul>
      </div>

      <hr />
      <h2>About Me</h2>
      <p>
        I am currently a Professor at
        <a href="http://cs.hitsz.edu.cn/">School of Computer Science and Technology</a>,
        <a href="https://www.hitsz.edu.cn/index.html">Harbin Institute of Technology (Shenzhen)</a>. Prior to that, I
        was a postdoc at Nanyang Technological University, Singapore, working with
        <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a> and
        <a href="https://www.mmlab-ntu.com/person/ccloy/">Prof. Chen Change Loy</a>.
      </p>
      <p>
        I received my PhD degree from
        <a href="http://www.comp.hkbu.edu.hk/v1/">Department of Computer Science</a>,
        <a href="https://www.hkbu.edu.hk/eng/main/index.jsp">Hong Kong Baptist University</a> in 2021, supervised by
        <a href="http://www.comp.hkbu.edu.hk/~pcyuen/">Prof. Pong C. Yuen</a>, and my bachelor degree from
        <a href="http://www.sice.uestc.edu.cn/">School of Information and Communication Engineering</a>,
        <a href="http://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in
        2015. I also spent a memorable high-school time in
        <a href="http://www.sfls.net.cn/">Shenzhen Foreign Languages School</a>. I visited Johns Hopkins University for
        6 months in 2020.
      </p>
      <p>
        <!-- I am interested in computer vision and multimodal learning. My current research focuses on Multimodal Large
        Language Models (MLLMs) (e.g.,
        <a href="https://github.com/JiuTian-VL">JiuTian MLLM</a>) and their agent applications (e.g., Embodied AI and
        GUI Agents).<br> -->

        I am interested in computer vision and multimodal learning. 
        My current research focuses on Multimodal Large Language Model (MLLM) (e.g., <a href="https://github.com/JiuTian-VL">JiuTian MLLM</a>)
        and its applications on Embodied AI.
      </p>

      <hr />
      <h2>Biography</h2>
      <ul>
        <li>2021–2023: Research Fellow, <a href="https://www.mmlab-ntu.com/index.html">MMLab@NTU</a>, Singapore</li>
        <li>2021.07–2021.11: Researcher, <a href="https://www.sensetime.com/en">SenseTime</a>, Shenzhen</li>
        <li>2017–2021: Ph.D., <a href="http://www.comp.hkbu.edu.hk/v1/">Department of Computer Science</a>, <a href="https://www.hkbu.edu.hk/">Hong Kong Baptist University</a></li>
        <li>2020.2–2020.7: Visiting Scholar, <a href="https://www.jhu.edu/">Johns Hopkins University</a></li>
        <li>2011–2015: B.S., <a href="http://www.sice.uestc.edu.cn/">School of Information and Communication Engineering</a>, <a href="http://www.uestc.edu.cn/">UESTC</a></li>
      </ul>

      <hr>
      <h2>Services</h2>
      <ul>
        <li>ICMR 2025 Panel Chairs</li>
        <li>Area Chair: ACM Multimedia 2024, BMVC 2024, BMVC 2025</li>
        <!--
        <li>Invited Reviewer for: TPAMI, IJCV, TIP, TNNLS, TIFS, TCSVT, JSTSP, PR</li>
        <li>Conferences: CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, IJCAI, ACM MM</li>
        <li>Program Committee: AAAI 2021, 2022, 2023</li>
        -->
      </ul>

      <!-- <hr>
      <h2>Collaborators</h2>
      <ul>
        <li><a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a>, Nanyang Technological University</li>
        <li><a href="https://engineering.jhu.edu/vpatel36/vishal-patel/">Prof. Vishal M. Patel</a>, Johns Hopkins University</li>
        <li><a href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl=en">Dr. Xiangyuan Lan</a>, Peng Cheng Laboratory</li>
        <li><a href="https://pages.jh.edu/~pperera3/">Dr. Pramuditha Perera</a>, Johns Hopkins University, AWS AI Lab</li>
      </ul> -->


    <hr>
    </div>

    <div class="text-center" style="margin-bottom: 20px">
      <small>&copy; 2025 OrionLab</small>
    </div>

    <script src="static/jquery.js"></script>
    <script src="static/bootstrap/js/bootstrap.js"></script>
  </body>
</html>
